Level 1: What BPC did Longformer achieve on text8 dataset?
Answer: Timeout
Time: 30.02s

Level 1: How many tokens can Longformer process compared to BERT's 512 limit?
Answer: 4,096 tokens (8 times BERT's 512) 
Time: 9.41s

Level 1: What window size does Longformer use for local attention?
Answer: 512 
Time: 7.62s

Level 2: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: Longformer: O(n) linear memory scaling Full self-attention: O(nÂ²) quadratic memory scaling 
Time: 7.24s

Level 2: What are the key differences between sliding window and dilated sliding window attention?
Answer: We do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. attention Pattern Sliding Window Given the importance of local context (Kovaleva et al., 2019) (Wu et al., 2019) 2 b Dilated Sliding Window To further increase the receptive field without increasing computation, the sliding window can be "dilated" for Longformer 
Time: 25.90s

Level 2: How does Longformer initialize position embeddings beyond RoBERTa's 512 limit?
Answer: Copies RoBERTa's 512 position embeddings multiple times 
Time: 7.83s

Level 3: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: This task is considered one of the fundamental tasks in natural language and recent prior work on modeling long sequences using transformers has relied on this task as their primary evaluation (Dai et al., 2019; Rae et al., 2020; Sukhbaatar et al., 2019) attention Pattern for autoregressive language modeling we use our dilated sliding window attention. 
Time: 23.78s

Level 3: What is the relationship between Longformer's attention pattern design and its computational efficiency gains?
Answer: this section discusses the design and implementation of this attention pattern. this section discusses the design and implementation of this attention pattern. this section discusses the design and implementation of this attention pattern. this section discusses the design and implementation of this attention pattern. 
Time: 17.02s

Level 3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. experiment Setup To compare to prior work we focus on characterlevel LM (text8 and enwik8; Mahoney, 2009) training Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. the dataset does not provide any intermediate annotation for the multi 
Time: 21.45s

Level 4: What evidence supports that both local and global attention components are essential for Longformer's performance?
Answer: Ablation studies show both attention types are essential 
Time: 7.37s

Level 4: How do the ablation study results validate the architectural choices made in Longformer's design?
Answer: To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each configuration for 150 K steps 4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set. 8 shows that our Longformer-large achieves new state-of-the-art results 9 on Wiki Hop and Trivia QA by 
Time: 21.56s

Level 4: What are the computational trade-offs between different Longformer implementations (loop vs chunks vs CUDA)?
Answer: Longformer-loop is a naive implementation that computes each diagonal separately in a loop. It is memory efficient because it only computes the non-zero values, but it is unusably slow. We only use it for testing because it is easy to implement but don't use it to run experiments. Longformer-chunks only supports the nondilated case. It chunks Q and K into overlapping blocks of size 
Time: 23.69s

Level 5: Based on the experimental setup and results, what are the potential limitations of Longformer for real-time applications?
Answer: Key limitations: - It is memory efficient because it only computes the non-zero values, but it is unusably slow. - It is memory efficient because it only computes the non-zero values, but it is unusably slow. 
Time: 7.70s

Level 5: How might the evaluation methodology bias the conclusions about Longformer's effectiveness compared to other approaches?
Answer: We then keep up to 5 paragraphs whose raw score is higher than a pre-specified threshold (-3.0), and remove the other paragraphs from the context. 
Time: 14.02s

Level 5: What aspects of long document understanding are NOT adequately addressed by Longformer's approach?
Answer: Autoregressive language modeling (LM), while the application of long document transformers to documentlevel NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) Longformer's attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encode 
Time: 23.29s

