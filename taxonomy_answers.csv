level,question,answer,mode_used,debug_info
1,What datasets did Longformer achieve state-of-the-art results on?,text 8 and enwik 8 bench mark,ext,Processing question: What datasets did Longformer achieve state-of-the-art results on? | Retrieved 10 relevant chunks
1,What does Longformer's attention mechanism scale linearly with?,the sequence length,ext,Processing question: What does Longformer's attention mechanism scale linearly with? | Retrieved 10 relevant chunks
2,What are the two types of attention that Longformer combines?,"windowed attention used during pretraining, plus a task mot iva ted global attention",ext,Processing question: What are the two types of attention that Longformer combines? | Retrieved 10 relevant chunks
2,What is the main limitation of Transformer-based models for long sequences?,memory req uir eme nts,ext,Processing question: What is the main limitation of Transformer-based models for long sequences? | Retrieved 10 relevant chunks
3,How does Longformer's memory usage compare to full self-attention?,relative small difference,hybrid,Processing question: How does Longformer's memory usage compare to full self-attention? | Retrieved 10 relevant chunks | Extracted 8 relevant facts
3,What can Longformer process that other models cannot?,long seq uen ces,hybrid,Processing question: What can Longformer process that other models cannot? | Retrieved 10 relevant chunks | Extracted 8 relevant facts
4,What specific advantages does Longformer have over RoBERTa for long document tasks?,LEDâ€™s encoder reads the document and its decoder ge nerates the output summary.,gen,Processing question: What specific advantages does Longformer have over RoBERTa for long document tasks? | Retrieved 10 relevant chunks | Extracted 8 relevant facts
4,"How do the different Longformer implementations (loop, chunk, cuda) compare in terms of speed?","Longformer can process long seq uen ces without chunking, allowing us to adopt a much simpler approach that con cate nates the available context and pro ces ses it in a single pass.",gen,"Processing question: How do the different Longformer implementations (loop, chunk, cuda) compare in terms of speed? | Retrieved 10 relevant chunks | Extracted 8 relevant facts"
5,What potential improvements could be made to Longformer for even longer documents?,"Longformer-base has a maximum sequence length of 4,096 sequences, while Ro BERTa-base only has 256 sequences.",gen,Processing question: What potential improvements could be made to Longformer for even longer documents? | Retrieved 10 relevant chunks | Extracted 8 relevant facts
5,How might Longformer's approach influence future transformer architectures?,"In the case of Longformer, we use global attention to the entire question and answer candidate sequence",gen,Processing question: How might Longformer's approach influence future transformer architectures? | Retrieved 10 relevant chunks | Extracted 8 relevant facts
