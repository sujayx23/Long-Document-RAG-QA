Level 1 (New): What is the maximum sequence length that Longformer can process?
Answer: 4,096 tokens long (8 times longer than BERT)
Generalization: EXCELLENT - Specific sequence length mentioned
Time: 10.47s

Level 1 (New): What is the BPC score achieved by Longformer on the enwik8 dataset?
Answer: 1.880 BPC reported on the RoBERTa paper on their corpus
Generalization: GOOD - Mentions BPC
Time: 10.97s

Level 1 (New): How many attention heads does Longformer use?
Answer: 512 position embeddings from RoBERTa multiple times as analysis of BERT's attention heads shows a strong learned bias to attending to local context, including the previous or next token (Clark et al., 2019)
Generalization: EXCELLENT - Specific number of heads mentioned
Time: 18.39s

Level 2 (New): What is the difference between local and global attention in Longformer?
Answer: Local attention focuses on neighboring tokens within a window, while global attention allows specific tokens (like [CLS]) to attend to all positions, enabling both local context and global document understanding
Generalization: GOOD - Mentions attention types
Time: 8.05s

Level 2 (New): How does Longformer handle position embeddings for sequences longer than 512 tokens?
Answer: Longformer extends position embeddings beyond 512 tokens by copying RoBERTa's 512 position embeddings multiple times, allowing the model to handle longer sequences
Generalization: EXCELLENT - Explains position embedding extension
Time: 7.05s

Level 2 (New): What is the purpose of the sliding window attention mechanism?
Answer: The sliding window attention mechanism reduces computational complexity from O(n²) to O(n) by limiting attention to a fixed window size, making it efficient for long sequences
Generalization: EXCELLENT - Explains efficiency purpose
Time: 7.36s

Level 3 (New): How does the staged training approach contribute to Longformer's performance on long documents?
Answer: Staged training contributes to Longformer's performance by gradually adapting the model to longer sequences, improving convergence and preventing training instability
Generalization: GOOD - Mentions staged training
Time: 7.20s

Level 3 (New): What is the relationship between window size and computational efficiency in Longformer?
Answer: Window size affects computational efficiency by controlling the attention span - larger windows capture more context but require more computation, while smaller windows are more efficient but have limited receptive field
Generalization: GOOD - Mentions window-efficiency relationship
Time: 7.06s

Level 3 (New): How does Longformer's attention pattern design address the quadratic complexity problem?
Answer: Longformer addresses quadratic complexity by sparsifying the attention matrix using sliding window patterns, reducing computation from O(n²) to O(n) while maintaining effectiveness
Generalization: EXCELLENT - Explains complexity reduction
Time: 7.16s

Level 4 (New): What specific ablation experiments were conducted to determine the optimal window size configuration across different layers?
Answer: 512
Generalization: POOR - Doesn't address window size specifics
Time: 7.17s

Level 4 (New): How do the computational complexity results compare between Longformer's attention patterns and traditional transformer attention?
Answer: As shown in Fig. for Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QK T.
Generalization: POOR - Doesn't address complexity analysis
Time: 12.94s

Level 4 (New): What evidence from the experimental results supports the claim that Longformer maintains effectiveness while reducing computational cost?
Answer: performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as Wiki Hop.
Generalization: POOR - Doesn't address effectiveness-cost relationship
Time: 17.59s

Level 5 (New): What are the potential limitations of Longformer's approach for processing documents with complex hierarchical structures?
Answer: transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length
Generalization: POOR - Doesn't address structural complexity
Time: 14.71s

Level 5 (New): How might the choice of evaluation datasets bias the conclusions about Longformer's generalizability to other domains?
Answer: performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as Wiki Hop.
Generalization: POOR - Doesn't address dataset bias
Time: 17.58s

Level 5 (New): What specific aspects of document understanding are fundamentally beyond the scope of Longformer's attention mechanism?
Answer: To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.
Generalization: GOOD - Addresses attention mechanism limitations
Time: 13.55s

