ENHANCED RAG SYSTEM REPORT
================================================================================
Date: 2025-07-28 11:34:19

Success Rate: 4/5 (80.0%)

ANSWERS:
--------------------------------------------------------------------------------

Level 1: What window size does Longformer use for local attention?
Answer: 2 d Linear Projections for Global attention recall that given the linear projections Q, K, V, the transformer model (Vaswani et al., 2017) attention (QK, V ) = softmax QK T  d k V (1) We use two sets of projections, Q S, K S, V S to compute attention scores of sliding window attention, and Q G, K G, V G to compute Attention scores for the global attention.
Success: False
Method: generated

Level 2: What are the key differences between sliding window and dilated sliding window attention?
Answer: For Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QK T. As shown in Fig.
Success: True
Method: generated

Level 2: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences.
Success: True
Method: generated

Level 3: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8.
Success: True
Method: generated

Level 3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases.
Success: True
Method: generated
