Level 1: What BPC did Longformer achieve on text8 dataset?
Answer: 1.10 BPC
Time: 8.36s

Level 1: How many tokens can Longformer process compared to BERT's 512 limit?
Answer: 4,096 tokens (8 times BERT's 512)
Time: 6.55s

Level 1: What window size does Longformer use for local attention?
Answer: 512
Time: 6.15s

Level 2: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.
Time: 17.95s

Level 2: What are the key differences between sliding window and dilated sliding window attention?
Answer: Sliding window attends to neighboring tokens while dilated sliding window has gaps for larger receptive field
Time: 7.41s

Level 2: How does Longformer initialize position embeddings beyond RoBERTa's 512 limit?
Answer: Copies RoBERTa's 512 position embeddings multiple times
Time: 6.53s

Level 3: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling.
Time: 18.95s

Level 3: What is the relationship between Longformer's attention pattern design and its computational efficiency gains?
Answer: Sliding window attention reduces computation from O(nÂ²) to O(n) while maintaining effectiveness
Time: 7.28s

Level 3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: Staged training increases window size and sequence length across multiple phases to learn local context first
Time: 7.09s

Level 4: What evidence supports that both local and global attention components are essential for Longformer's performance?
Answer: Ablation studies demonstrate that both local and global attention components are essential for performance
Time: 6.60s

Level 4: How do the ablation study results validate the architectural choices made in Longformer's design?
Answer: Ablation studies validate architectural choices by testing different attention pattern configurations
Time: 6.58s

Level 4: What are the computational trade-offs between different Longformer implementations (loop vs chunks vs CUDA)?
Answer: Loop implementation is memory efficient but slow, while chunked implementation balances speed and memory
Time: 6.58s

Level 5: Based on the experimental setup and results, what are the potential limitations of Longformer for real-time applications?
Answer: Some implementations are memory efficient but too slow for real-time applications
Time: 6.19s

Level 5: How might the evaluation methodology bias the conclusions about Longformer's effectiveness compared to other approaches?
Answer: Evaluation methodology may bias conclusions by using pre-specified thresholds for paragraph selection
Time: 6.42s

Level 5: What aspects of long document understanding are NOT adequately addressed by Longformer's approach?
Answer: Longformer focuses on single long documents but doesn't address: cross-document reasoning, structured data within documents (tables/graphs), or sequences exceeding GPU memory
Time: 6.43s

