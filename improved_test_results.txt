Level 1: What BPC did Longformer achieve on text8 dataset?
Answer: 1.10 BPC

Level 1: How many tokens can Longformer process compared to BERT's 512 limit?
Answer: 4,096 tokens (8 times BERT's 512)

Level 1: What window size does Longformer use for local attention?
Answer: 512

Level 2: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.

Level 2: What are the key differences between sliding window and dilated sliding window attention?
Answer: Sliding window attends to neighboring tokens while dilated sliding window has gaps for larger receptive field

Level 2: How does Longformer initialize position embeddings beyond RoBERTa's 512 limit?
Answer: Copies RoBERTa's 512 position embeddings multiple times

Level 3: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling.

Level 3: What is the relationship between Longformer's attention pattern design and its computational efficiency gains?
Answer: Sliding window attention reduces computation from O(n²) to O(n) while maintaining effectiveness

Level 3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: Staged training increases window size and sequence length across multiple phases to learn local context first

Level 4: What specific experimental results demonstrate the necessity of both local and global attention in Longformer?
Answer: through ablations and controlled trials we show both attention types are essential

Level 4: Which ablation study findings directly influenced the final architectural choices in Longformer’s design?
Answer: To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. 8 shows that our Longformer-large achieves new state-of-the-art results 9 on Wiki Hop and Trivia QA by large margins

Level 4: In practical terms, how do speed and memory usage differ between Longformer’s loop, chunk, and CUDA implementations?
Answer: this is very compute efficient because it uses a single matrix multiplication operation from Py Torch, but it consumes 2 x the amount of memory

Level 5: What are the main obstacles that prevent Longformer from being used in real-time applications, according to the reported experiments?
Answer: performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as Wiki Hop.

Level 5: In what ways could the evaluation methodology used in the Longformer paper lead to overestimating its effectiveness?
Answer: performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings

Level 5: Which specific challenges in long document understanding remain unsolved by Longformer, based on the authors’ discussion?
Answer: transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length

