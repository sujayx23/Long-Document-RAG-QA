Level 1: What BPC did Longformer achieve on text8 dataset?
Answer: 1.10 BPC

Level 1: How many tokens can Longformer process compared to BERT's 512 limit?
Answer: 4,096 tokens (8 times BERT's 512)

Level 1: What window size does Longformer use for local attention?
Answer: 512

Level 2: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.

Level 2: What are the key differences between sliding window and dilated sliding window attention?
Answer: Sliding window attends to neighboring tokens while dilated sliding window has gaps for larger receptive field

Level 2: How does Longformer initialize position embeddings beyond RoBERTa's 512 limit?
Answer: Copies RoBERTa's 512 position embeddings multiple times

Level 3: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling.

Level 3: What is the relationship between Longformer's attention pattern design and its computational efficiency gains?
Answer: Sliding window attention reduces computation from O(n²) to O(n) while maintaining effectiveness

Level 3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: Staged training increases window size and sequence length across multiple phases to learn local context first

Level 4: What specific experimental results demonstrate the necessity of both local and global attention in Longformer?
Answer: Ablation studies show removing global attention drops WikiHop accuracy by 8.3 points, and removing local attention significantly degrades performance on character-level tasks, demonstrating both are essential

Level 4: Which ablation study findings directly influenced the final architectural choices in Longformer’s design?
Answer: Ablation studies validate architectural choices by testing different attention pattern configurations

Level 4: In practical terms, how do speed and memory usage differ between Longformer’s loop, chunk, and CUDA implementations?
Answer: Implementation trade-offs: Loop implementation is memory efficient but slow, chunk implementation is fast but only supports non-dilated attention, and CUDA kernel provides the best balance of speed and features

Level 5: What are the main obstacles that prevent Longformer from being used in real-time applications, according to the reported experiments?
Answer: Main obstacles for real-time applications:

Level 5: In what ways could the evaluation methodology used in the Longformer paper lead to overestimating its effectiveness?
Answer: The evaluation may overestimate effectiveness because performance drops significantly when using pretrained RoBERTa embeddings without proper fine-tuning, suggesting the model relies heavily on task-specific training

Level 5: Which specific challenges in long document understanding remain unsolved by Longformer, based on the authors’ discussion?
Answer: Specific challenges not addressed by Longformer:

