Level 4: What specific experimental results demonstrate the necessity of both local and global attention in Longformer?
Answer: Ablation studies show removing global attention drops WikiHop accuracy by 8.3 points, and removing local attention significantly degrades performance on character-level tasks, demonstrating both are essential
Time: 7.28s

Level 4: Which ablation study findings directly influenced the final architectural choices in Longformer's design?
Answer: Ablation studies validate architectural choices by testing different attention pattern configurations
Time: 6.90s

Level 4: In practical terms, how do speed and memory usage differ between Longformer's loop, chunk, and CUDA implementations?
Answer: Implementation trade-offs: Loop implementation is memory efficient but slow, chunk implementation is fast but only supports non-dilated attention, and CUDA kernel provides the best balance of speed and features
Time: 6.89s

Level 5: What are the main obstacles that prevent Longformer from being used in real-time applications, according to the reported experiments?
Answer: Main obstacles for real-time applications:
Time: 7.09s

Level 5: In what ways could the evaluation methodology used in the Longformer paper lead to overestimating its effectiveness?
Answer: The evaluation may overestimate effectiveness because performance drops significantly when using pretrained RoBERTa embeddings without proper fine-tuning, suggesting the model relies heavily on task-specific training
Time: 6.72s

Level 5: Which specific challenges in long document understanding remain unsolved by Longformer, based on the authors' discussion?
Answer: Specific challenges not addressed by Longformer:
Time: 7.49s

