Comprehensive QA Test Results

Level 1: Single Chunk Information:
Q1: What BPC did Longformer achieve on text8 dataset?
Answer: We achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00
Time: 24.39s

Q2: How many tokens can Longformer process compared to BERT's 512 limit?
Answer: The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT) 5.
Time: 13.04s

Q3: What window size does Longformer use for local attention?
Answer: The Longformer variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention.
Time: 16.58s

Level 2: Adjacent Chunks Information:
Q1: How does Longformer's memory usage compare to full self-attention as sequence length increases?
Answer: To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.
Time: 22.59s

Q2: What are the key differences between sliding window and dilated sliding window attention?
Answer: The global attention uses additional linear projections ( 3.1).
Time: 21.70s

Q3: How does Longformer initialize position embeddings beyond RoBERTa's 512 limit?
Answer: by copying the 512 position embeddings from RoBERTa multiple times
Time: 20.55s

Level 3: Distant Chunks Information:
Q1: How do the character-level language modeling results relate to the downstream task performance improvements?
Answer: To do so, we pretrained Longformer on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution.
Time: 16.61s

Q2: What is the relationship between Longformer's attention pattern design and its computational efficiency gains?
Answer: The Longformer variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention.
Time: 30.21s

Q3: How does the staged training procedure for character LM connect to the pretraining approach for downstream tasks?
Answer: To do so, we pretrained Longformer on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution.
Time: 19.74s

Level 4: Cross-Section Analysis:
Q1: What evidence supports that both local and global attention components are essential for Longformer's performance?
Answer: Through ablations and controlled trials we show both attention types are essential -the local attention is primarily used to build contextual representations, while the global attention allows Longformer to building full sequence representation for prediction.
Time: 18.64s

Q2: How do the ablation study results validate the architectural choices made in Longformer's design?
Answer: To make the ablation study more manageable, we train each configuration for 150 K steps 4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set.
Time: 16.65s

Q3: What are the computational trade-offs between different Longformer implementations (loop vs chunks vs CUDA)?
Answer: No specific answer found in the document.
Time: 19.87s

Level 5: Document-Wide Reasoning:
Q1: Based on the experimental setup and results, what are the potential limitations of Longformer for real-time applications?
Answer: To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases.
Time: 20.25s

Q2: How might the evaluation methodology bias the conclusions about Longformer's effectiveness compared to other approaches?
Answer: To make the ablation study more manageable, we train each configuration for 150 K steps 4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set.
Time: 18.18s

Q3: What aspects of long document understanding are NOT adequately addressed by Longformer's approach?
Answer: full quadratic attention matrix multiplication
Time: 18.83s

